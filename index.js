import { createHash } from "node:crypto";
import { createReadStream } from "node:fs";
import { spawn } from "node:child_process";
import { onExit } from "signal-exit";
import _pg from "pg";
const { Client } = _pg;
import { ok } from "node:assert";

const DIGEST_LENGTH = 6;

const digestString = (str) => {
	return createHash("sha256")
		.update(str)
		.digest("hex")
		.slice(0, DIGEST_LENGTH)
		.toLowerCase();
};

const digestFile = async (pathToFile) => {
	const hash = createHash("sha256");
	for await (const chunk of createReadStream(pathToFile)) {
		hash.update(chunk);
	}
	return hash.digest("hex").slice(0, DIGEST_LENGTH).toLowerCase();
};

const pSpawn = (path, args = [], opts = {}) => {
	return new Promise((resolve, reject) => {
		const proc = spawn(path, args, opts);
		// https://github.com/sindresorhus/execa/blob/f4b8b3ab601c94d1503f1010822952758dcc6350/lib/kill.js#L95-L101
		const stopListening = onExit(() => {
			proc.kill();
		});
		proc.once("error", (err) => {
			reject(err);
			stopListening();
			proc.kill();
		});
		proc.once("exit", (code, signal) => {
			if (code === 0) {
				resolve();
			} else {
				const err = new Error(
					`${path} exited with ${code} (${signal})`
				);
				err.code = code;
				err.signal = signal;
				err.proc = proc;
				reject(err);
			}
			stopListening();
		});
	});
};

const formatSchemaName = ({ importedAt }) => {
	return `gtfs_${importedAt}`;
};

const getPgConfig = async (cfg) => {
	const { pgHost, pgPort, pgUser, pgPassword, pgMetaDatabase, pgOpts } = {
		pgHost: null,
		pgPort: null,
		pgUser: null,
		pgPassword: null,
		pgMetaDatabase: null,
		pgOpts: {},
		...cfg,
	};

	const pgConfig = {
		...pgOpts,
	};
	if (pgHost !== null) {
		pgConfig.host = pgHost;
	} else if (process.env.PGHOST) {
		pgConfig.host = process.env.PGHOST;
	}
	if (pgPort !== null) {
		pgConfig.port = pgPort;
	} else if (process.env.PGPORT) {
		pgConfig.port = process.env.PGPORT;
	}
	if (pgUser !== null) {
		pgConfig.user = pgUser;
	} else if (process.env.PGUSER) {
		pgConfig.user = process.env.PGUSER;
	}
	if (pgPassword !== null) {
		pgConfig.password = pgPassword;
	} else if (process.env.PGPASSWORD) {
		pgConfig.password = process.env.PGPASSWORD;
	}
	if (pgMetaDatabase !== null) {
		pgConfig.database = pgMetaDatabase;
	} else if (process.env.PGDATABASE) {
		pgConfig.database = process.env.PGDATABASE;
	}

	return pgConfig;
};

const connectToMetaDatabase = async (cfg) => {
	const pgConfig = await getPgConfig(cfg);
	const db = new Client(pgConfig);
	await db.connect();

	return db;
};

// https://www.postgresql.org/docs/15/libpq-connect.html#id-1.7.3.8.3.5
const getPgEnv = async (pgConfig) => {
	const pgEnv = {};

	if (pgConfig.host !== null) {
		pgEnv.PGHOST = pgConfig.host;
	}
	if (pgConfig.port !== null) {
		pgEnv.PGPORT = pgConfig.port;
	}
	if (pgConfig.user !== null) {
		pgEnv.PGUSER = pgConfig.user;
	}
	if (pgConfig.password !== null) {
		pgEnv.PGPASSWORD = pgConfig.password;
	}
	if (pgConfig.database !== null) {
		pgEnv.PGDATABASE = pgConfig.database;
	}
	// todo: ssl mode?

	return pgConfig;
};

const successfulImportsTableName = "latest_successful_imports";

const ensureSuccesfulImportsTableExists = async (cfg) => {
	const { db } = cfg;
	ok(cfg.db, "missing/empty cfg.db");

	await db.query(`\
		CREATE TABLE IF NOT EXISTS public.${successfulImportsTableName} (
			schema_name TEXT PRIMARY KEY,
			imported_at INTEGER NOT NULL,
			feed_digest TEXT NOT NULL
		)
	`);
};

const queryImports = async (cfg) => {
	let db;
	if ("db" in cfg) {
		ok(cfg.db, "missing/empty cfg.db");
		db = cfg.db;
	} else {
		db = await connectToMetaDatabase(cfg);
	}

	let latestSuccessfulImports = [];
	let allSchemas = [];
	try {
		const { rows: _rows } = await db.query(`\
			SELECT
				schema_name,
				imported_at,
				feed_digest
			FROM public.${successfulImportsTableName}
			WHERE schema_name LIKE 'gtfs\_%'
			ORDER BY imported_at DESC
		`);
		latestSuccessfulImports = _rows.map((row) => ({
			schemaName: row.schema_name,
			importedAt: row.imported_at,
			feedDigest: row.feed_digest,
		}));
	} catch (err) {
		if (
			err.message !==
			`relation "${successfulImportsTableName}" does not exist`
		) {
			throw err;
		}
	}

	{
		const { rows: _rows } = await db.query(`\
			SELECT
				nspname AS schema_name
			FROM pg_catalog.pg_namespace
			WHERE nspname LIKE 'gtfs\_%'
			ORDER BY nspname ASC
		`);
		allSchemas = _rows.map((row) => row.schema_name);
	}

	return {
		latestSuccessfulImports,
		allSchemas,
	};
};

const recordSuccessfulImport = async (cfg) => {
	const {
		db,
		successfulImport: { schemaName, importedAt, feedDigest },
	} = cfg;
	ok(db, "missing/empty cfg.db");
	ok(schemaName, "missing/empty cfg.successful.schemaName");
	ok(importedAt, "missing/empty cfg.successful.importedAt");
	ok(feedDigest, "missing/empty cfg.successful.feedDigest");

	await db.query(
		`\
			INSERT INTO public.${successfulImportsTableName} (schema_name, imported_at, feed_digest)
			VALUES ($1, $2, $3)
		`,
		[schemaName, importedAt, feedDigest]
	);
};

const removeSchemaFromLatestSuccessfulImports = async (cfg) => {
	const { db, schemaName } = cfg;
	ok(db, "missing/empty cfg.db");
	ok(schemaName, "missing/empty cfg.schemaName");

	await db.query(
		`\
			DELETE FROM public.${successfulImportsTableName}
			WHERE schema_name = $1
		`,
		[schemaName]
	);
};

export {
	digestString,
	digestFile,
	pSpawn,
	formatSchemaName,
	getPgEnv,
	getPgConfig,
	connectToMetaDatabase,
	successfulImportsTableName,
	ensureSuccesfulImportsTableExists,
	queryImports,
	recordSuccessfulImport,
	removeSchemaFromLatestSuccessfulImports,
};
